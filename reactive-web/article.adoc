= Going Reactive with WebFlux

== I/O, I/O, It's Off to Work We Go...

Reactive programming is an approach to writing software that embraces asynchronous IO. Asynchronous I/O is a small idea that portends big changes for software. The idea is simple: alleviate inefficient resource utilization by reclaiming resources that would otherwise be idle as they waited for I/O activity. Asynchronous IO inverts the normal design of IO processing: the clients are notified of new data instead of asking for it; this frees the client to do other things while waiting for new notifications. Let's look at an example that compares and contrasts asynchronous IO to synchronous IO.

Let's build a simple program that reads data from a source (a `java.io.File` reference, specifically). First up, an implementation that uses a trusty 'ol `java.io.InputStream` implementation:


.Read data from a file _synchronously_
====
[source,java,indent=0]
----
include::./src/main/java/com/example/io/Synchronous.java[]
----
<1> source the file using a regular `java.io.File`
<2> _pull_ the results out of the source one line at a time..
<3> I've written this code to accept a `Consumer<BytesPayload>` that gets called when there's new data
====

Pretty straightforward, eh? Run this and you'll see in the log output, on the left hand side of each line, that all activity is happening on a single thread.

We're _pulling_ bytes out of a source of data (in this case, a `java.io.InputStream` subclass, `java.io.FileInputStream`). What's wrong with this example? Well, probably nothing! In this case we're using an `InputStream` that's pointing to data on the local file system. If the file is there, and the hard drive is working, then this code will work as we expect.

What if, instead of reading data from a `File`, we read data from a network socket, and used a different implementation of an `InputStream`? Nothing to worry about! Well, nothing to worry about if the network is infinitely fast, at least. And if the network link between this node and another never fails. If those things are true, then there's certainly nothing to worry about! This code will work just fine..

What happens if the network is slow, or down? In this case, it'd mean that the time it takes for the `in.read(...)` operation to return would be prolonged. Indeed, it may never return! This is a problem if we're trying to do something else with the thread on which we're reading data. Sure, we can spin up another thread and read from that one instead. We could keep this up to a point, but eventually we'll run into a limit where adding threads doesn't support our goal of scaling. We won't have true concurrency beyond the number of cores on our machine. We're stuck! We can't handle more I/O, reads in this case, without adding threads, and our ability to scale up with more threads is, ultimately, limited.

In that example, the bulk of the work is in the reading - there's not much else going on anywhere. We are __I/O bound_. Let's see how an asynchronous solution can help us alleviate the monopolization of our threads.

.Read data from a file _asynchronously_
====
[source,java,indent=0]
----
include::src/main/java/com/example/io/Asynchronous.java[]
----
<1> this time, we adapt the `java.io.File` into a Java NIO `java.nio.file.Path`
<2> when we create the `Channel`, we specify, among other things, a `java.util.concurrent.ExecutorService`, that will be used to invoke our `CompletionHandler` when there's data available.
<3> start reading, passing in a reference to a `CompletionHandler<Integer, ByteBuffer>` (`this`).
<4> in the callback, we read the bytes out of  a `ByteBuffer` into a `byte[]` holder.
<5> Just as in the `Synchronous` example, the `byte[]` data is passed to a consumer.
====

First thing's first: this code's _waaaay_ more complicated! There's a ton of things going on here and it can seem overwhelming, but indulge me, for a moment... This code  reads data from a Java NIO `Channel` and processes that data, asynchronously, on a separate thread in a callback handler. The thread on which the read was started isn't monopolized. We return virtually instantly after we call `.read(..)`, and when there is finally data available, our callback is invoked, and on a different thread. If there is latency between `.read()` calls, then we can move on and do other things with our thread. The duration of the asynchronous read, from the first byte to the last, is at best as short as the duration of the synchronous read. It's likely a tiny bit longer. But, for that complexity, we can be more efficient with our threads. We can handle more work, multiplexing I/O across a finite thread pool.

I work for a cloud computing company. We'd  _love_ it if you solved your scale-out problems by buying more application instances! Of course I'm being a bit tongue-in-cheek  here. Asynchronous IO __does_ make things a bit more complicated, but hopefully this example highlights the ultimate benefit of reactive code: we can handle more requests, and do more work, using asynchronous I/O on the same hardware _if_ our work is IO bound. If it's CPU-bound  (e.g.: fibonacci, bitcoin mining, or cryptography) then reactive programming won't buy us anything.

Now, most of us don't work with `Channel` _or_ `InputStream` implementations for their day-to-day work! They think about things in terms of higher order abstractions. Things like the arrays, or, more likely, the `java.util.Collection` hierarchy. A `java.util.Collection` maps very nicely to an `InputStream`: they both   assume that you'll be able to work with all the data, near instantly. You expect to be able to finish reading from most `InputStreams` sooner rather than later.  Collection types start to become a bit awkward when you move to larger sums of data; what happens when you're dealing with something potentially infinite - unbounded - like websockets, or server-sent events? What happens when there's latency between records? One record arrives now and another not for another minute or hour  such as with a chat, or when the network suffers a failure?


We need a better way to describe these kinds of data. We're describing something asynchronous - something that will _eventually_ happen. This might seem a good fit for a `Future<T>` or a `CompletableFuture<T>`, but that only describes _one_ eventual thing. Not a whole stream of potentially unlimited things. Java hasn't really offered an appropriate metaphor by which to describe this kind of data.  Both `Iterator` and Java 8 `Stream` types can be unbounded, but they are both pull-centric; you ask for the next record instead of having the type call your code back. One assumes that if they did support push-based processing, which lets you do more with your threads, that the APIs would also expose threading and scheduling control. `Iterator` implementations say nothing about threading and Java 8 streams _all_ share the same fork-join pool.

If `Iterator` and `Stream` did support push-based processing, then we'd run into another problem that really only becomes an issue in the context of IO: we'd need some way to  _push back_!  As a consumer of data being produced asynchronously, we have no idea when or how much data might be in the pipeline. We don't know if one byte will be produced in the next callback or a if terabyte will be produced! When you pull data off of an `InputStream`, you read as much data as you're prepared to handle, and no more. In the examples above we read into a `byte[]`  buffer of a fixed and known length. In an asynchronous world, we need someway to communicate to the producer how much data we're prepared to handle.

Yep. We're _definitely_ missing something.


== The Missing Metaphor

What we want is something that maps nicely to asynchronous I/O, and that supports this push-back mechanism, or  _flow control_, in distributed systems. In reactive programming, the ability of the client to signal how much work it can manage tis called _backpressure_. There are a good deal many projects -  Vert.x, Akka Streams, and RxJava - that support reactive programming. The Spring team has a project called http://projectreactor.io[Reactor]. There's common enough ground across these different approaches extracted into a de-facto standard, http://www.reactive-streams.org[the Reactive Streams initiative]. The Reactive Streams initiative defines four types:

The `Publisher<T>` is a producer of values that may eventually arrive. A `Publisher<T>` produces values of type `T` to a `Subscriber<T>`.

.the Reactive Streams `Publisher<T>`.
====
[source,java,indent=0]
----
package org.reactivestreams;

public interface Publisher<T> {

    void subscribe(Subscriber<? super T> s);
}
----
====

The `Subscriber` subscribes to a `Publisher<T>`, receiving notifications on any new values of type `T` through its `onNext(T)` method. If there are any errors, its `onError(Throwable)` method is called. When processing has completed normally, the subscriber's `onComplete` method is called.


.the Reactive Streams `Subscriber<T>`.
====
[source,java,indent=0]
----
package org.reactivestreams;

public interface Subscriber<T> {

    public void onSubscribe(Subscription s);

    public void onNext(T t);

    public void onError(Throwable t);

    public void onComplete();
}
----
====

When a `Subscriber` first connects to a `Publisher`, it is given a `Subscription` in the `Subscriber#onSubscribe` method. The `Subscription` is arguably the most important part of the whole specification: it enables backpressure. The `Subscriber` uses the `Subscription#request` method to request more data or the `Subscription#cancel` method to halt processing.

.The Reactive Streams `Subscription<T>`.
====
[source,java,indent=0]
----
package org.reactivestreams;

public interface Subscription {

    public void request(long n);

    public void cancel();
}
----
====

The Reactive Streams specification provides _one_ more useful, albeit obvious, type:  A `Processor<A,B>`  is a simple interface that extends both `Subscriber<A>` and a `Publisher<B>`.

.The Reactive Streams `Processor<T>`.
====
[source,java,indent=0]
----
package org.reactivestreams;

public interface Processor<T, R> extends Subscriber<T>, Publisher<R> {
}
----
====



The specification is not meant to be a prescription for the implementations,   instead defining types for interoperability. The Reactive Streams types are so obviously useful that they __eventually_ found their way into the recent Java 9 release as one-to-one semantically equivalent interfaces in the `java.util.concurrent.Flow` class, e.g.: `java.util.concurrent.Flow.Publisher`.


== Reactor

The Reactive Streams types are not enough; you'll need higher order implementations to support operations like filtering and transformation. Pivotal's Reactor project is a good choice here; it builds on top of the Reactive Streams specification. It provides two specializations of   `Publisher<T>`. The first, `Flux<T>`, is a `Publisher` that produces zero or more values. It's unbounded. The second, `Mono<T>`, is a `Publisher<T>` that produces zero or one value. They're both publishers and you can treat them that way, but they go much further than the  Reactive Streams specification. They both provide operators, ways to  process a stream of values. Reactor types compose nicely - the output of one thing can be the input to another and if a type needs to work with other streams of data, they rely upon `Publisher<T>` instances.

Both `Mono<T>` and `Flux<T>` implement `Publisher<T>`; our recommendation is that your methods accept `Publisher<T>` instances but return `Flux<T>` or `Mono<T>` to help the client distinguish the kind of data its being given. Suppose you're given a `Publisher<T>` and asked to render a user-interface for that `Publisher<T>`. Should you render a detail page for one record, as you might were you given a `CompletableFuture<T>`? Or should you render an overview page, with a list or grid displaying _all_ the records in a paged fashion? It's hard to know. `Flux<T>` and `Mono<T>`, on the other hand, are very specific. You know to render an overview page if you're given a `Flux<T>` and a detail page for one (or no) record when given a `Mono<T>`.

Reactor is a Pivotal project; it's become very popular. Facebook use it in their https://github.com/rsocket/rsocket-java[reactive RPC mechanism, RSocket], led by RxJava creator Ben Christensen. Salesforce use it in their https://github.com/salesforce/reactive-grpc[reactive gRPC implementation]. It implements the Reactive Streams types, and so can interoperate with other technologies that support those types like https://github.com/ReactiveX/RxJava/blob/2.x/src/main/java/io/reactivex/Flowable.java[Netflix's RxJava 2], https://doc.akka.io/docs/akka/current/stream/operators/Sink/asPublisher.html#aspublisher[Lightbend's Akka Streams], and https://vertx.io/docs/vertx-reactive-streams/java/[the Eclipse Foundation's Vert.x project]. David Karnok, lead of RxJava 2, has worked extensively with Pivotal on Reactor, too, making it even better.  And, of course, it's been in Spring Framework in some form or another since Spring Framework 4.0.

== Reactive Spring

As useful as project Reactor is, it's only a foundation. Our applications need to talk to data sources. They need to produce and consume HTTP, SSE and WebSocket endpoints. They will need to support authentication and authorization. Spring provides these things. If Reactor gives us the missing metaphor, Spring helps us all speak the same language.

Spring Framework 5.0 was released in September 2017. It builds on Reactor and the Reactive Streams specification. It includes a new reactive runtime and component model called https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/web-reactive.html#webflux[Spring WebFlux]. Spring WebFlux does not depend on or require the Servlet APIs to work. It ships with adapters that allow it to work on top of a Servlet-engine, if need be, but it's not required. It also provides a net new Netty-based web server. Spring Framework 5, which works with a baseline of Java 8 and Java EE 7,  is now the baseline for much of the Spring ecosystem including Spring Data Kay, Spring Security 5, Spring Boot 2 and Spring Cloud Finchley.

== Getting Started

Let's build something! We'll begin our journey, as usual, at my second http://start.Spring.io[favorite place on the internet, the Spring Initializr -  start.Spring.io]. The goal here is to build a new reactive web application that supports reactive data access, and then secure it (reactively!). Select the following dependencies either by using the combo box on the bottom right of the page or by selecting "Switch to the Full Version" and then choosing `DevTools`, `Reactive Web`, `Reactive MongoDB`. and `Lombok`.


.Selections on the Spring Initializr for a new, reactive application.
image::./images/the-spring-initializr.png[alt=the Spring Initializr,width=1000]

This will give you a new project with the following layout.

.The generated project structure.
====
[source,java,indent=0]
----
include::includes/tree.txt[test]
----
====

Our Maven build file, `pom.xml`, is pretty plain, but it assumes we're going to use JUnit 4. Let's upgrade JUnit to use JUnit 5, which is a more modern testing framework that's well supported by Spring Framework 5 and beyond. This owes in no small part to the fact that the lead of JUnit 5, Sam Brennan, is also the lead of the Spring Test framework. Add the following dependencies to your new application's build file, `pom.xml`: `org.junit.jupiter`:`junit-jupiter-engine` and give it a `scope` of `test`. Then, exclude the `junit`:`junit` dependency from the `spring-boot-starter-test` dependency. This is   the resulting Maven `pom.xml`:

.`pom.xml`
====
[source,java,indent=0]
----
include::pom.xml[pom.xml]
----
====

This is a stock-standard Spring Boot application with a `public static void main(String [] args)` entry-point class, `DemoApplication.java`:

.`src/main/java/com/example/demo/DemoApplication.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/DemoApplication.java[]
----
====

There's also an empty configuration file, `src/main/resources/application.properties`.

We're ready to get started! Let's turn to the first concern, data access.

== Reactive Data Access

We want to talk to a natively reactive data store. That is, the driver for the database needs to itself support asynchronous IO, otherwise we won't be able to scale out reads without scaling out threads, which defeats the point. Spring Data, an umbrella data access framework, supports a number of reactive data access options including reactive Cassandra, reactive MongoDB, reactive Couchbase and reactive Redis. We've chosen MongoDB, so make sure you have a MongoDB database instance running on your local machine on the default host, port, and accessible with the default username and password. If you're on a Mac, you can use `brew install mongodb`.

MongoDB is a document database, so the unit of interaction is a sparse document - think of it as a JSON stanza that gets persistd and is retreivable by a key, the document ID.

Our application will support manipulating `Profile` objects. We're going to persist `Profile` entities (reactively) using a reactive Spring Data repository, as documents in MongoDB. Let's first look at the entity definition. It's got one field, `email`, that will be persisted in MongoDB, and another field that will act as the document ID.


.`src/main/java/com/example/demo/Profile.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/Profile.java[]
----
<1> `@Document` identifies the entity as a document to be persisted in MongoDB
<2> `@Data`, `@AllArgsConstructor`, and `@NoArgsConstructor` are all from Lombok. They're compile-time  annotations that tell Lombok to synthesize getters/setters, constructors, a `toString`  method and an `equals` method.
<3> `@Id` is a Spring Data annotation that identifies the document ID for this document
<4> ..and finally, this field `email` is the thing that we want to store and retreive later.
====


In order to persist documents of type `Profile`, we declaratively define a repository. A repository, a design pattern from Eric Evans' seminal tome, _Domain Driven Design_, is a way of encapsulating  object persistence. Repositories are responsible for persisting entities and value types. They present clients with a simple model for obtaining persistent objects and managing their life cycle. They decouple application and domain design from persistence technology and strategy choices. They also communicate design decision sabout object access. And, finally, they allow easy substitiion of implementation with a dummy implemenetation, ideal in testing. Spring Data's repositories support all these goals with interface definitions whose implementation are satisfied by the framework at startup time.

Let's look at our trivial Spring Data repisitory, `src/main/java/com/example/demo/ProfileRepository.java`.

.`src/main/java/com/example/demo/ProfileRepository.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/ProfileRepository.java[]
----
====


Our repository extends the Spring Data-provided `ReactiveMongoRepository` interface which in turn provides a number of data access methods supporting reads, writes, deletes and searches, almost all in terms of method signatures accepting or returning `Publisher<T>` types.

.`org.springframework.data.mongodb.repository.ReactiveMongoRepository`
====
[source,java,indent=0]
----
package org.springframework.data.mongodb.repository;

import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import org.reactivestreams.Publisher;
import org.springframework.data.domain.Example;
import org.springframework.data.domain.Sort;
import org.springframework.data.repository.NoRepositoryBean;
import org.springframework.data.repository.query.ReactiveQueryByExampleExecutor;
import org.springframework.data.repository.reactive.ReactiveSortingRepository;

@NoRepositoryBean
public interface ReactiveMongoRepository<T, ID> extends ReactiveSortingRepository<T, ID>, ReactiveQueryByExampleExecutor<T> {

	<S extends T> Mono<S> insert(S entity);

	<S extends T> Flux<S> insert(Iterable<S> entities);

	<S extends T> Flux<S> insert(Publisher<S> entities);

	<S extends T> Flux<S> findAll(Example<S> example);

	<S extends T> Flux<S> findAll(Example<S> example, Sort sort);

}
----
====

Spring Data will create an object that implements all these methods. It will provide an object for us that we can inject into into other objects to handle persistence.   If you define an empty repository, as we have, then there's little reason to test the repository implementation. It'll "just work."

Spring Data repositories also supports custom queries. We could, for example, define a custom finder method, of the form `Flux<Profile> findByEmail(String email)`, in our `ProfileRepository`  and  this would result in a method being defined that looks for all documents in MongoDB with a predicate that matches the `email` attribute in the document to the parameter, `email`, in the method name. If you define custom queries, then this might be an appropriate thing to test.

This is a sample application, of course, so we need some sample data with which to work. Let's run some initialization logic when the application starts up. We can define a bean of type `ApplicationListener<ApplicationReadyEvent>` to be a consumer of an  `ApplicationContextEvent`, `ApplicationReadyEvent`, when the application starts us. This will be an enviable opportunity for us to write some sample data into the databse once the application's started up.

.`src/main/java/com/example/demo/SampleDataInitializer.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/SampleDataInitializer.java[]
----
<1> a Lombok annotation that results in the creation of a `log` field that is a Log4J logger being added to the class
<2> this bean initializes sample data that is only useful for a demo. We don't want this sample data being initialized every time. Spring's `Profile` annotation tags an object for initialization only when the profile that matches the profile specified in the annotation is specifically activated.
<3> we'll use the `ProfileRepository` to handle persistence
<4> here we start a reactive pipeline by first deleting everything in the databse. This operation returns a `Mono<T>`. Both `Mono<T>` and `Flux<T>` support chanining processing with the `thenMany(Publisher<T>)` method. So, after the `deleteAll()` method completes, we then want to process the writes of new data to the datbase.
<5>  we use  Reactor's `Flux<T>.just(T...)` factory method to create a new `Publisher` with a static list of `String` records, in-memory..
<6> ..and we transform each record in turn into a `Profile` object..
<7> ..that we then persist to the databse using our repository
<8> After all the data has been written to the database, we want to fetch all the records from the database to confirm what we have there
<9> If we'd stopped at the previous line, the `save` operation, and run this program then we would see.. nothing! `Publisher<T>`  instances are _lazy_ - you need to `subscribe()` to them to trigger their execution. This last line is where the rubber meets the road. In this case, we're using the `subscribe(Consumer<T>)` variant that lets us visit every record returned from the `repository.findAll()` operation and print out the record.
====

You'll note that in the previous example we use two methods, `map(T)` and `flatMap(T)`. Map should be familiar if you've ever used the Java 8 `Stream` API. Map visits each record in a publisher and passes it through a lambda function which must transform it. The output of that transformation is then returned and accumulated into a new `Publisher`. So, the intermediate type after we return from our `map` operation is a `Publisher<Profile>`. In the next line we then call `flatMap`. `flatMap` is just like `map`, except that it unpacks the return value of the lambda given if the value is itself contained in a `Publisher<T>`. In our case, the `repository.save(T)` method returns a `Mono<T>`. If we'd used `.map` instead of `flatMap(T)`, we'd have a `Flux<Mono<T>>`, when what we really want is a `Flux<T>`. We can cleanly solve this problem usig `flatMap`.

== A Reactive Service

We're going to use the repository to implement a service that will contain any course grained business logic. In the beginning a lot of the business logic will be pass through logic delegating to the repository, but we can add things like validation and integration with other systems at this layer. Let's look at a simple service.


.`src/main/java/com/example/demo/ProfileService.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/ProfileService.java[]
}
----
<1> This tells Lombok to create a `log` field that is a Log4J logger.
<2> we'll want to publish events to other beans managed in the Spring `ApplicationContext`. Earlier, we defined an `ApplicationListener<ApplicationReadyEvent>` that listened for an event that was published in the `ApplicationContext`. Now, we're going to publish an event for consumption of other beans of our devices in the `ApplicationContext`.
<3> we defer to our repository to
<4> ..find all documents or..
<5> ..find a document by its ID..
<6> ..update a `Profile` and give it a new `email`..
<7> ..delete a record by its `id`..
<8> ..or create a new `Profile` in the database and publish an `ApplicationContextEvent`, one of our own creation called `ProfileCreatedEvent`, on successful write to the database. The `doOnSuccess` callback takes a `Consumer<T>` that gets invoked after the data in the reactive pipeline has been written to the database. We'll see later why this event is so useful.
====

The `ProfileCreatedEvent` is just like any other Spring `ApplicationEvent`.

.`src/main/java/com/example/demo/ProfileCreatedEvent.java`
====
[source,java,indent=0]
----
include::src/main/java/com/example/demo/ProfileCreatedEvent.java[]
----
====

That wasn't so bad, was it? Our service was pretty straightforward. The only novelty was the publishing of an event. Everything should be working just fine now. But, of course, we can't possibly know that unless we test it.

=== Testing our Reactive Service


Reactive code presents some subtle issues when testing. Remember, our code is asynchronous. It's possibly concurrent. Each `Subscriber<T>` could execute on a different thread because the pipeline is managed by a `Scheduler`. You can change which scheduler is to be used by calling `(Flux,Mono).subscribeOn(Scheduler)`. There's a convenient factory, `Schedulers.\*`, that lets you build a new `Scheduler` from, for example, a `java.util.concurrent.Executor`. You don't normally need to override the `Scheduler`, though. By default there's one thread per core and the scheduler will just work. You only really need to worry about it when the thing to which you're subscribing could end up blocking. If, for example, you end up making a call to a blocking JDBC datastore in your `Publisher<T>`, then you should scale up interactions with that datastore with more threads using a `Scheduler`.

You need to understand that the `Scheduler` is present because it implies asynchronocity. This asynchronicity and concurrency is deterministic if you use the operators in the Reactor API: things  _will_ execute as they should. It's only ever problematic, or inscrutable, when attempting to poke at the state of the reactive pipeline from outside. Then things get a bit twisted.  Reactor ships with some very convenient testing support that allow you to assert things about reactive `Publisher<T>` instances - what is going to be created and when - without having to worry about the schedulers. Let's look at some tests using the `StepVerifier` facility.

In order for us to appreciate what's happening here, we need to take a moment and step back and revisit _test slices_. Test slices are a feature in Spring Boot that allow the client to laod the types in a Spring `ApplicationContext` that are adjacent to the thing under test. In this case, we're interested in testing the data access logic in the service. We are _not_ interested in testing the web functionality. We haven't even written the web functionality yet, for a start! A test slice lets us tell Spring Boot to load nothing by defdault and then we can bring pieces back in iteratively.

When Spring Boot starts up it runs a slew of auto-configuration classes. Classes that produce objects that Spring in turn manages for us. The objects are provided by default assuming certain conditions are met. These conditions can include all sorts of things, like the presence of certain types on the classpath, properties in Spring's `Environment`, and more. When a Spring Boot application starts up, it is the sum of all the auto-configurations and user configuration given to it. It will be, for our application, database connectivity, object-record mapping (ORM), a webserver, and so much more.

We only need the machinery related to  MongoDB and our `ProfileService`, in isolation. We'll use the  `@DataMongoTest` annotation to tell Spring Boot to autoconfigure all the things that could be implied in our MongoDB logic, while ignoring things like the web server,  runtime and web components. This results in focused, faster test code that has the benefit of being easier to reproduce. The `@DataMongoTest` annotation is what's called a _test slice_ in the Spring Boot world. It supports testing a _slice_ of our application's functionality in isolation. There are numerous other test slices and you can easily create your own, too.

Test slices can also contribute _new_ auto-configuration supporting tests, specifically. The `@DataMongoTest` does this. It can even run an _embedded_ MongoDB instance using the Flapdoodle library! Let's take advantage of this. Add the following to your Maven build `de.flapdoodle.embed` : `de.flapdoodle.embed.mongo` with scope `test`. There's no need to specify a version; Spring Boot will manage that for us.


.`src/test/java/com/example/demo/ProfileServiceTest.java`
====
[source,java,indent=0]
----
include::src/test/java/com/example/demo/ProfileServiceTest.java[]
----
<1> the Spring Boot test slice for MongoDB testing
<2> we want to add, in addition to all the MongoDB functionality, our custom service for testing
<3> This annotation tells JUnit 5 to involve the `SpringExtension` class when running this test. `SpringExtension` in turn manages instances of the class under test. We can easily inject dependencies from Sprign into our test classes. We can even inject them into the constructor! The extension is what integrates Spring with JUnit 5.
<4> Look ma! Constructor injection in a unit test!
<5> Make sure you're using the new `org.junit.jupiter.api.Test` annotation from JUnit 5..
<6> In this unit test we setup state in one publisher (`saved`)..
<7> ..and then assert things about that state in the various `expectNextMatches` calls.
<8> Make sure to call `verifyComplete`! Otherwise, nothing'll happen.. and that makes me sad.
====


We only annotated one test because the rest are unremarkable and similiar.

The `StepVerifier` is central to testing all things reactive. It gives us a way to assert that what we think is going to come next in the publisher is in fact going to come next in the publisher. The `StepVerifier` provides several variants on the `expect*` theme. Think of this as the reactive equivalent to `Assert*`.

JUnit 5 supports the same lifecycle methods and annotations (like `@Before`) as JUnit 4. This is great because it gives you a single place to set up all tests in a class, or to tear down the machinery between tests. In reactive tests, however, those  That said, I wouldn't _subscribe_ to any reactive initialization pipelines in the `setUp`  method. Instead, you might define  a `Flux<T>` in the setup, and then compose it in the body of the test methods. This way, you don't have to wonder if the setup has concluded before the tests themselves execute.

